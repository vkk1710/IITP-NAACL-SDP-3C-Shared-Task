{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install jsonlines","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install semanticscholar","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!pip install transformers\n#!wget -O scibert_uncased.tar https://s3-us-west-2.amazonaws.com/ai2-s2-research/scibert/huggingface_pytorch/scibert_scivocab_uncased.tar\n#!tar -xvf scibert_uncased.tar\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport jsonlines\nimport semanticscholar as sch\nimport os\nimport re\nimport torch\nimport math\nimport random\nfrom sklearn.metrics import f1_score\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom tqdm import tqdm\n\nfrom transformers import AutoTokenizer, AutoModel\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom collections import Counter\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score\nfrom IPython.display import FileLink,FileLinks","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# scicite data loading............\ntrain_main = '../input/dataset/scicite/train.jsonl'\ntest_main = '../input/dataset/scicite/test.jsonl'\nval_main = '../input/dataset/scicite/dev.jsonl'\nsec_sc = '../input/dataset/scicite/scaffolds/sections-scaffold-train.jsonl'\ncit_sc = '../input/dataset/scicite/scaffolds/cite-worthiness-scaffold-train.jsonl'","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **V3 model training on SciCite**","metadata":{}},{"cell_type":"markdown","source":"# 1. **SciCite Data**","metadata":{}},{"cell_type":"code","source":"## Section scaffold data\np=[]\nwith jsonlines.open(sec_sc) as f:\n    for line in f.iter():\n        p.append(line)\ndata = pd.DataFrame(p)\ndata","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Citation Worthiness scaffold data\nq=[]\nwith jsonlines.open(cit_sc) as f:\n    for line in f.iter():\n        q.append(line)\ndatac = pd.DataFrame(q)\ndatac","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## SciCite Main task data\ntm=[]\nwith jsonlines.open(train_main) as f:\n    for line in f.iter():\n        tm.append(line)\ndata_main = pd.DataFrame(tm)\ndata_main","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. **Data Preprocessing**","metadata":{}},{"cell_type":"code","source":"def remove_idx(l,max_len): # max_len = len of tokens criteria above which to remove\n    lent = []\n    idx_remove = []\n    for i in l:\n        lent.append(len(tokenizer.encode(i, padding=True, return_tensors=\"pt\")[0]))\n    for i in range(len(lent)):\n        if(lent[i]>max_len):\n            idx_remove.append(i)\n    return idx_remove","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# returns balanced dataset.....\ndef class_div(X,Y,instances,num_class):\n    inst_per_class = instances//num_class\n    rem = instances%num_class\n    x_out=[]\n    y_out=[]\n    for i in range(num_class):\n        xcl = [x for x,y in zip(X,Y) if y==i]\n        ycl = [y for y in Y if y==i]\n        print('xcl length : ',len(xcl))\n        if(rem):\n            x_out += xcl[:inst_per_class+1]\n            y_out += ycl[:inst_per_class+1]\n            rem -= 1\n            print('x_out length : ',len(x_out))\n        else:\n            x_out += xcl[:inst_per_class]\n            y_out += ycl[:inst_per_class]\n            print('x_out length : ',len(x_out))\n    return (x_out,y_out)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fun(i):\n    pat = ' '.join(re.split(r'\\[[^\\[\\]]*\\]' ,i))\n    pat = ' '.join(re.split(r'\\([^\\[\\]\\(\\)]*\\)' ,pat))\n    return pat","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***SciCite Train Data***","metadata":{}},{"cell_type":"code","source":"citm = data_main['string'].values.tolist()\ncitm = [fun(i) for i in citm]\ndata_main['processed_text'] = citm\nlen(citm)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"yc = data_main['label'].values.tolist()\nlabel={'background':0,'method':1,'result':2}\ny = list(map(lambda t : label[t],yc))\nlen(y)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"allenai/scibert_scivocab_uncased\",do_lower_case=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"citm_idx_remove_300 = remove_idx(citm,300)\nfor i in sorted(citm_idx_remove_300, reverse = True):  \n    del citm[i]\n    del y[i]\nprint(f'removed {len(citm_idx_remove_300)} instances')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_cit = datac['cleaned_cite_text'].values.tolist()\nx_cit = [fun(i) for i in x_cit]\ndatac['processed_text'] = x_cit\nlen(x_cit)\n\nycit = datac['is_citation'].values.tolist()\nycit_ie = list(map(lambda x : int(x),ycit))\nlen(ycit_ie)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xcit_idx_remove_300 = remove_idx(x_cit,300)\nfor i in sorted(xcit_idx_remove_300, reverse = True):  \n    del x_cit[i]\n    del ycit_ie[i]\nprint(f'removed {len(xcit_idx_remove_300)} instances')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_sec = data['text'].values.tolist()\nx_sec = [fun(i) for i in x_sec]\ndata['processed_text'] = x_sec\nlen(x_sec)\n\ny_sec = data['section_name'].values.tolist()\nlabels={'introduction':0,'related work':1,'method':2,'experiments':3,'conclusion':4}\nysec_ie = list(map(lambda t : labels[t],y_sec))\nlen(ysec_ie)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xsec_idx_remove_300 = remove_idx(x_sec,300)\nfor i in sorted(xsec_idx_remove_300, reverse = True):  \n    del x_sec[i]\n    del ysec_ie[i]\nprint(f'removed {len(xsec_idx_remove_300)} instances')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#for v-1 model = method - 1 = without balancing the data\ntokenizer = AutoTokenizer.from_pretrained(\"allenai/scibert_scivocab_uncased\",do_lower_case=True)\n\nx = list(map(lambda t : tokenizer.encode(t,padding='max_length',max_length=300),citm))\n\ny = torch.tensor(y[:-2])\nx = torch.tensor(x[:-2])\nprint('x.shape : ',x.shape)\nprint('y.shape : ',y.shape)\n\nxcit = list(map(lambda t : tokenizer.encode(t,padding='max_length',max_length=300),x_cit))\n\nxsec = list(map(lambda t : tokenizer.encode(t,padding='max_length',max_length=300),x_sec))\n\nsec = [(x,y) for x,y in zip(xsec,ysec_ie)]\nsec = random.sample(sec, 8232)\n\ncit = [(x,y) for x,y in zip(xcit,ycit_ie)]\ncit = random.sample(cit, 8232)\n\nxcit = torch.tensor([t[0] for t in cit])\nycit = torch.tensor([t[1] for t in cit])\nxsec = torch.tensor([t[0] for t in sec])\nysec = torch.tensor([t[1] for t in sec])\nprint(xcit.shape)\nprint(xsec.shape)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***SciCite Val Data***","metadata":{}},{"cell_type":"code","source":"# validation data\nvm=[]\nwith jsonlines.open(val_main) as f:\n    for line in f.iter():\n        vm.append(line)\npdvm = pd.DataFrame(vm)\npdvm        ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"citvm=pdvm['string'].values.tolist()\ncitvm = [fun(i) for i in citvm]\npdvm['processed_text'] = citvm\nyc = pdvm['label'].values.tolist()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label={'background':0,'method':1,'result':2}\nvy = list(map(lambda t : label[t],yc))\ntokenizer = AutoTokenizer.from_pretrained(\"allenai/scibert_scivocab_uncased\",do_lower_case=True)\nvx = list(map(lambda t : tokenizer.encode(t,padding='max_length',max_length=300),citvm))\nvy = torch.tensor(vy)\nvx = torch.tensor(vx)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vy.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***SciCite Test Data***","metadata":{}},{"cell_type":"code","source":"# test data\ntestm=[]\nwith jsonlines.open(test_main) as f:\n    for line in f.iter():\n        testm.append(line)\npdtm = pd.DataFrame(testm)\npdtm        ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cittem=pdtm['string'].values.tolist()\ncittem = [fun(i) for i in cittem]\npdtm['processed_text'] = cittem\ncittem = cittem[:-1]\nyc = pdtm[:-1]['label'].values.tolist()\nlabel={'background':0,'method':1,'result':2}\nty = list(map(lambda t : label[t],yc))\ntokenizer = AutoTokenizer.from_pretrained(\"allenai/scibert_scivocab_uncased\",do_lower_case=True)\ntx = list(map(lambda t : tokenizer.encode(t,padding='max_length',max_length=300),cittem))\nty = torch.tensor(ty)\ntx = torch.tensor(tx)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. **Model v3 architecture**","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class attention(nn.Module):\n    def __init__(self, feature_dim, step_dim, bias=True, **kwargs):\n        super(attention, self).__init__(**kwargs)\n        \n        self.supports_masking = True\n\n        self.bias = bias\n        self.feature_dim = feature_dim\n        self.step_dim = step_dim\n        self.features_dim = 0\n        self.a = 0\n        self.th = 0\n        self.eij = 0\n        \n        weight = torch.zeros(feature_dim, 1)\n        nn.init.kaiming_uniform_(weight)\n        self.weight = nn.Parameter(weight)\n        print('weight : ',self.weight)\n        print('weight shape : ',weight.shape)\n        \n        if bias:\n            self.b = nn.Parameter(torch.zeros(step_dim))\n        print('bias shape : ',self.b.shape)\n        \n    def forward(self, x, mask=None):\n        feature_dim = self.feature_dim \n        step_dim = self.step_dim\n#         print('x shape : ',x.shape)\n\n        self.eij = torch.mm(\n            x.contiguous().view(-1, feature_dim), \n            self.weight\n        ).view(-1, step_dim)\n        \n#         print('eij shape : ',self.eij.shape)\n        \n        if self.bias:\n            self.eij = self.eij + self.b\n            \n        self.th = torch.tanh(self.eij)\n#         print('tanh out shape : ',self.th.shape)\n        a = torch.exp(self.th)\n#         print('a shape : ',a.shape)\n        \n        if mask is not None:\n            a = a * mask\n\n        self.a = a / (torch.sum(a, 1, keepdim=True) + 1e-10)\n#         print('a divided by sum shape : ',self.a.shape)\n\n        weighted_input = x * torch.unsqueeze(self.a, -1)\n#         print('weighted input : ',weighted_input.shape)\n        return torch.sum(weighted_input, 1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#feed forward of main task\nclass feedforward1(nn.Module):\n    def __init__(self,data):\n        super().__init__()\n        \n        n = 3 if data=='sci' else 6\n        drop = 0.2 if data == 'sci' else 0.4\n            \n        self.drop = nn.Dropout(p=drop)\n        self.lin = nn.Linear(100,20)\n        self.relu = torch.nn.ReLU()\n        self.out = nn.Linear(20,n)\n    def forward(self,x):\n        x = self.drop(x)\n        lin_out = self.lin(x)\n        x = self.relu(lin_out)\n        x = self.out(x)\n        return (x,lin_out) ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#feed forward of section scaffold\nclass feedforward2(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.drop = nn.Dropout(p=0.4)\n        self.lin = nn.Linear(100,20)\n        self.relu = torch.nn.ReLU()\n        self.out = nn.Linear(20,5)\n    def forward(self,x):\n        x = self.drop(x)\n        \n        lin_out = self.lin(x)\n        x = self.relu(lin_out)\n        x = self.out(x)\n        return (x,lin_out) ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#feed forward of citation worthiness scaffold\nclass feedforward3(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.drop = nn.Dropout(p=0.4)\n        self.lin = nn.Linear(100,20)\n        self.relu = torch.nn.ReLU()\n        self.out = nn.Linear(20,2)\n    def forward(self,x):\n        x = self.drop(x)\n        lin_out = self.lin(x)\n        x = self.relu(lin_out)\n        x = self.out(x)\n        return (x,lin_out) ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#feed forward of citance + cited title scaffold\nclass feedforward4(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.drop = nn.Dropout(p=0.4)\n        self.lin = nn.Linear(100,20)\n        self.relu = torch.nn.ReLU()\n        self.out = nn.Linear(20,6)\n    def forward(self,x):\n        x = self.drop(x)\n        lin_out = self.lin(x)\n        x = self.relu(lin_out)\n        x = self.out(x)\n        return (x,lin_out) ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class modelv3(nn.Module):\n    def __init__(self,batch_size):\n        super().__init__()\n        self.batch_size = batch_size\n        self.BertModel = AutoModel.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n        self.lstm = nn.LSTM(768,50,num_layers=1,bidirectional=True,batch_first=True)\n        self.att = attention(100,300)   ## for attention defination\n        self.main_sci = feedforward1(data='sci')\n        self.main_pk = feedforward1(data='pk')\n        self.sec = feedforward2()\n        self.cit = feedforward3()\n        self.cited = feedforward4()\n    def forward(self,x,n,data=None):\n        xbert=self.BertModel(x)\n        lstm,_=self.lstm(xbert[0])\n        at = self.att(lstm).view(-1,100)\n        if(n==1 and data=='sci'):\n            return self.main_sci(at)[0]\n        \n        elif(n==1 and data=='pk'):\n            return self.main_pk(at)[0]\n        \n        elif(n==2):\n            return self.sec(at)[0]\n        \n        elif(n==3):\n            return self.cit(at)[0]\n        elif(n==4):\n            return self.cited(at)[0]\n        else:\n            # predicting == training done!!\n            if(data=='sci'):\n                z,last = self.main_sci(at)\n                _,last_sec = self.sec(at)\n                _,last_cit = self.cit(at)\n                z = F.softmax(z,dim=1)\n                z = torch.argmax(z,dim=1) \n                return (z,last,at,last_cit,last_sec,lstm)\n            else:\n                z,last = self.main_pk(at)\n                _,last_sec = self.sec(at)\n                _,last_cit = self.cit(at)\n                _,last_cited = self.cited(at)\n                z = F.softmax(z,dim=1)\n                z = torch.argmax(z,dim=1)\n                return (z,last,at,last_cit,last_sec,last_cited,lstm)\n        ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. **Training v3 model**","metadata":{}},{"cell_type":"markdown","source":"***Loading DataLoaders......***","metadata":{}},{"cell_type":"code","source":"batchSize = 12\nnum_of_feedforwards = 3\n#model = AutoModel.from_pretrained(\"allenai/scibert_scivocab_uncased\")\nmain_tr = torch.utils.data.TensorDataset(x, y)\nmain_val = torch.utils.data.TensorDataset(vx, vy)\nmain_test = torch.utils.data.TensorDataset(tx, ty)\nsec_tr = torch.utils.data.TensorDataset(xsec,ysec)\ncit_tr = torch.utils.data.TensorDataset(xcit,ycit)\n#tit_tr = torch.utils.data.TensorDataset(x_tit,y_tit)\n\n \ntrain_sampler = torch.utils.data.RandomSampler(main_tr)\ntrain_data = torch.utils.data.DataLoader(main_tr, sampler=train_sampler, batch_size=batchSize//num_of_feedforwards)\n\ntrain_sampler = torch.utils.data.RandomSampler(sec_tr)\nsec_data = torch.utils.data.DataLoader(sec_tr, sampler=train_sampler, batch_size=batchSize//num_of_feedforwards)\n\ntrain_sampler = torch.utils.data.RandomSampler(cit_tr)\ncit_data = torch.utils.data.DataLoader(cit_tr, sampler=train_sampler, batch_size=batchSize//num_of_feedforwards)\n\nval_sampler = torch.utils.data.RandomSampler(main_val)\nval_data = torch.utils.data.DataLoader(main_val, sampler=val_sampler, batch_size=batchSize//num_of_feedforwards)\n\ntest_sampler = torch.utils.data.RandomSampler(main_test)\ntest_data = torch.utils.data.DataLoader(main_test, sampler=test_sampler, batch_size=batchSize//num_of_feedforwards)\n\n# train_sampler = torch.utils.data.RandomSampler(tit_tr)\n# tit_data = torch.utils.data.DataLoader(tit_tr, sampler=train_sampler, batch_size=batchSize//num_of_feedforwards)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***Training.....***","metadata":{}},{"cell_type":"code","source":"## Training models\nmod=modelv3(batchSize//num_of_feedforwards)\nmod.to(device)\n\nfor name,param in mod.named_parameters():\n    if(name.split('.')[0] == 'main_pk' or name.split('.')[0] == 'cited'):\n        param.requires_grad = False\n\nlambd1 = 0.05   #lambd1 for influence of section scaffold\nlambd2 = 0.1    #lambd2 for influence of citation worthiness scaffold\n#lambd3 = 0.1    #lambd3 for influence of cited paper title scaffold\n\nloss = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adadelta(mod.parameters(), lr = 0.01)\n\nloss_list = []\nf1_list = []\n\nfor epoch in range(20):\n    running_loss = 0\n    ypr=[]\n    ytr=[]\n    y_eval=[]\n    ypr_eval=[]\n    \n# training--------------------\n    mod = mod.train()\n#     print(mod.att.att_weights.requires_grad)\n#     print(mod.att.out.requires_grad)\n    for i,data in enumerate(zip(train_data,cit_data,sec_data)):\n        m = data[0]\n        c = data[1]\n        s = data[2]\n        \n        in_main,tar_main = m[0].to(device),m[1].to(device)\n        in_cit,tar_cit = c[0].to(device),c[1].to(device)\n        in_sec,tar_sec = s[0].to(device),s[1].to(device)\n        \n        optimizer.zero_grad()\n        \n        main = mod(in_main,1,'sci')\n        sec = mod(in_sec,2)\n        cit = mod(in_cit,3)\n        \n        loss_main = loss(main,tar_main)\n        loss_cit = loss(cit,tar_cit)\n        loss_sec = loss(sec,tar_sec)\n\n        overall_loss = (loss_main + lambd1*loss_sec + lambd2*loss_cit)/num_of_feedforwards  # becoz initially the summation is avg loss per mini batch(8) but we need avg loss per mini batch(24)\n        overall_loss.backward()\n        torch.nn.utils.clip_grad_norm_(mod.parameters(), 5)\n#         plot_grad_flow(mod.named_parameters())\n        optimizer.step()\n        \n        running_loss += overall_loss.item()\n        if(i%100 == 99):\n            loss_list.append({'epoch':epoch+1,'batch':i+1,'loss':round(running_loss / 100,3)})\n            print('[epoch : %d,batch : %5d] loss: %.3f' %(epoch + 1, i + 1, running_loss / 100))\n\n            running_loss = 0.0\n   \n# validation ------------------------\n    mod = mod.eval()\n    with torch.no_grad():\n        \n        # calculating f1_score for train data\n        for d in train_data:\n            x = d[0].to(device)\n            y = d[1].to(device)\n            for yt in y.cpu():\n                ytr.append(yt)\n            y_pred = mod(x,0,'sci')[0].cpu()\n            for yt in y_pred:\n                ypr.append(yt)\n                \n        f1 = f1_score(ytr,ypr,average='macro')\n        \n        # calculating f1_score for validation data\n        for d in val_data:\n            xv = d[0].to(device)\n            yv = d[1].to(device)\n            for yt in yv.cpu():\n                y_eval.append(yt)\n            y_pred = mod(xv,0,'sci')[0].cpu()\n            for yt in y_pred:\n                ypr_eval.append(yt)\n                \n        val_f1 = f1_score(y_eval,ypr_eval,average='macro')\n        \n        f1_list.append({'epoch':epoch+1,'train_f1_score':f1,'val_f1_score':val_f1})\n        \n    print('*'*40)\n    print('train confusion matrix : ')\n    print(confusion_matrix(ytr, ypr))\n    print('*'*40)\n    print('val confusion matrix : ')\n    print(confusion_matrix(y_eval, ypr_eval))\n    print('*'*40)\n    print('[epoch : %d] train_f1_macro: %.3f, val_f1_macro: %.3f' %(epoch+1, f1, val_f1))\n    print('*'*40)\n    torch.save(mod, f'./cohan_v3_newatt_dropout0.4_ep{epoch+1}.pt')\n    \n#     if((epoch+1)%2==0):\n#         torch.save(mod, f'./cohan_modelv3_ep{epoch+1}.pt')\n\nprint('Finished Training!!')  ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import FileLink, FileLinks\nFileLinks('./') #lists all downloadable files on server","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***Testing***","metadata":{}},{"cell_type":"code","source":"mod = torch.load('../input/v3models-trained-on-scicite/modelv3_scaffold4_dropout_0.4_new3c_smote_ep6.pt')\n# mod.eval()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## test on SciCite val data\ny_val=[]\nypr_val=[]\nwith torch.no_grad():\n    mod.eval()\n    for v in val_data:\n        xtv = v[0].to(device)\n        ytv = v[1].to(device)\n        for yv in ytv.cpu():\n            y_val.append(yv)\n        y_predv = mod(xtv,0,'sci')[0].cpu()\n        for yv in y_predv:\n            ypr_val.append(yv)\n                \nval_f1 = f1_score(y_val,ypr_val,average='macro')\nprint('val_f1_score : ',val_f1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## test on SciCite test data\ny_test=[]\nypr_test=[]\nwith torch.no_grad():\n    mod.eval()\n    for d in test_data:\n        xte = d[0].to(device)\n        yte = d[1].to(device)\n        for yt in yte.cpu():\n            y_test.append(yt)\n        y_pred = mod(xte,0,'sci')[0].cpu()\n        for yt in y_pred:\n            ypr_test.append(yt)\n                \ntest_f1 = f1_score(y_test,ypr_test,average='macro')\nprint('test_f1_score : ',test_f1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **V3 Model finetuning on 3C**","metadata":{}},{"cell_type":"code","source":"pdtr_new = pd.read_csv('../input/kaggle-3c-new/kaggle_3C_new/SDP_train.csv')\npdte_new = pd.read_csv('../input/kaggle-3c-new/kaggle_3C_new/SDP_test.csv')\npdtr_new","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# code for combining citance and cited title\n\nprtr_new = list(map(lambda t : t[:t.find('#AUTHOR_TAG')]+t[t.find('#AUTHOR_TAG')+11:],pdtr_new['citation_context']))\nprtr_new = [fun(i) for i in prtr_new]\npdtr_new['processed_text'] = prtr_new\nyc = pdtr_new['citation_class_label'].values.tolist()\ntokenizer = AutoTokenizer.from_pretrained(\"allenai/scibert_scivocab_uncased\",do_lower_case=True)\nxe = list(map(lambda t : tokenizer.encode(t),prtr_new))\nx_citede = list(map(lambda t : tokenizer.encode(t[0],padding=True,pad_to_max_length=True,max_length=251-len(t[1]))[1:],zip(pdtr_new['cited_title'],xe)))\nx1 = [x+y for x,y in zip(xe,x_citede)]\ny = torch.tensor(yc)\nx1 = torch.tensor(x1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = list(map(lambda t : tokenizer.encode(t,padding=True,pad_to_max_length=True,max_length=250),prtr_new))\nx = torch.tensor(x)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# only oversampling of minority classes done for now!!!\nsampling_strategy = {0:1447,1:600,2:600,3:600,4:600,5:600}  # use this if u want to undersample using RandomUnderSampler after oversampling by SMOTE\ntr_over = SMOTE(sampling_strategy=sampling_strategy)\ntr_under = RandomUnderSampler(sampling_strategy={0:600,1:600,2:600,3:600,4:600,5:600})\nx_smote,y_smote = tr_over.fit_sample(x,y)\nx_smote,y_smote = tr_under.fit_sample(x_smote,y_smote)\n\nx1_smote,y_smote = tr_over.fit_sample(x1,y)\nx1_smote,y_smote = tr_under.fit_sample(x1_smote,y_smote)\nprint(Counter(y_smote))\n\nsampling_strategy = {0:201,1:100,2:100,3:100,4:100,5:100} # use this if u want to undersample as well after oversampling by SMOTE is done\n# while using above dict for sampling strategy of SMOTE, put k_neighbors = 4\nval_over = SMOTE(sampling_strategy=sampling_strategy,k_neighbors = 3)\nval_under = RandomUnderSampler(sampling_strategy={0:100,1:100,2:100,3:100,4:100,5:100})\nvx_smote,vy_smote = val_over.fit_sample(vx,vy)\nvx_smote,vy_smote = val_under.fit_sample(vx_smote,vy_smote)\nprint(Counter(vy_smote))\n\nx_smote = torch.tensor(x_smote)\ny_smote = torch.tensor(y_smote)\nvx_smote = torch.tensor(vx_smote)\nvy_smote = torch.tensor(vy_smote)\nx1_smote = torch.tensor(x1_smote)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batchSize = 8\nnum_of_feedforwards = 2  # 2 for finetuning on 3C and 3 for training on SciCite.\n\nmain_tr = torch.utils.data.TensorDataset(x_smote, y_smote)\nmain_val = torch.utils.data.TensorDataset(vx_smote, vy_smote)\ntit_tr = torch.utils.data.TensorDataset(x1_smote,y_smote)\n \ntrain_sampler = torch.utils.data.RandomSampler(main_tr)\ntrain_data = torch.utils.data.DataLoader(main_tr, sampler=train_sampler, batch_size=batchSize//num_of_feedforwards)\n\nval_sampler = torch.utils.data.RandomSampler(main_val)\nval_data = torch.utils.data.DataLoader(main_val, batch_size=batchSize//num_of_feedforwards)\n\ntrain_sampler = torch.utils.data.RandomSampler(tit_tr)\ntit_data = torch.utils.data.DataLoader(tit_tr, sampler=train_sampler, batch_size=batchSize//num_of_feedforwards)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#training cell..................\nmod = torch.load('../input/cohan-v3-models/cohan_modelv3_dropout0.4_ep5.pt')\nmod.to(device)\n\nfor name,param in mod.named_parameters():\n    if(name.split('.')[0] == 'main_sci'):\n        param.requires_grad = False\n    if(name.split('.')[0] == 'main_pk' or name.split('.')[0] == 'cited'):\n        param.requires_grad = True\n\nlambd1 = 0.05   #lambd1 for influence of section scaffold\nlambd2 = 0.1    #lambd2 for influence of citation worthiness scaffold\nlambd3 = 0.1    #lambd3 for influence of cited paper title scaffold\n\nloss = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adadelta(mod.parameters(), lr = 0.01)\n\nloss_list = []\nf1_list = []\n\nfor epoch in range(30):\n    running_loss = 0\n    ypr=[]\n    ytr=[]\n    y_eval=[]\n    ypr_eval=[]\n    \n# training--------------------\n    mod.train()\n    \n    for i,data in enumerate(zip(train_data,tit_data)):\n        m = data[0]\n        t = data[1]\n        \n        in_main,tar_main = m[0].to(device),m[1].to(device)\n        in_tit,tar_tit = t[0].to(device),t[1].to(device)\n        \n        optimizer.zero_grad()\n        \n        main = mod(in_main,1,'pk')\n        tit = mod(in_tit,4)\n    \n        loss_main = loss(main,tar_main)\n        loss_tit = loss(tit,tar_tit)\n\n        overall_loss = (loss_main + lambd3*loss_tit)/num_of_feedforwards  # becoz initially the summation is avg loss per mini batch(8) but we need avg loss per mini batch(24)\n        overall_loss.backward()\n        torch.nn.utils.clip_grad_norm_(mod.parameters(), 5)\n        optimizer.step()\n        \n        running_loss += overall_loss.item()\n        if(i%100 == 99):\n            loss_list.append({'epoch':epoch+1,'batch':i+1,'loss':round(running_loss / 100,3)})\n            print('[epoch : %d,batch : %5d] loss: %.3f' %(epoch + 1, i + 1, running_loss / 100))\n\n            running_loss = 0.0\n   \n# validation ------------------------\n    with torch.no_grad():\n        mod.eval()\n        \n        # calculating f1_score for train data\n        for d in train_data:\n            x = d[0].to(device)\n            y = d[1].to(device)\n            for yt in y.cpu():\n                ytr.append(yt)\n            y_pred = mod(x,0,'pk')[0].cpu()\n            for yt in y_pred:\n                ypr.append(yt)\n                \n        f1 = f1_score(ytr,ypr,average='macro')\n        \n        # calculating f1_score for validation data\n        for d in val_data:\n            xv = d[0].to(device)\n            yv = d[1].to(device)\n            for yt in yv.cpu():\n                y_eval.append(yt)\n            y_pred = mod(xv,0,'pk')[0].cpu()\n            for yt in y_pred:\n                ypr_eval.append(yt)\n                \n        val_f1 = f1_score(y_eval,ypr_eval,average='macro')\n        \n        f1_list.append({'epoch':epoch+1,'train_f1_score':f1,'val_f1_score':val_f1})\n        \n    print('*'*40)\n    print('train confusion matrix : ')\n    print(confusion_matrix(ytr, ypr))\n    print('*'*40)\n    print('val confusion matrix : ')\n    print(confusion_matrix(y_eval, ypr_eval))\n    print('*'*40)\n    print('[epoch : %d] train_f1_macro: %.3f, val_f1_macro: %.3f' %(epoch+1, f1, val_f1))\n    print('*'*40)\n    if((epoch+1)%2==0):\n        torch.save(mod, f'./cohan_modelv3_scaffold4_dropout_0.4_3c_smote_ep{epoch+1}.pt')\n\nprint('Finished Training!!')            ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Submission file generation\n\nprte = list(map(lambda t : t[:t.find('#AUTHOR_TAG')]+t[t.find('#AUTHOR_TAG')+11:],pdte_new['citation_context']))\ntokenizer = AutoTokenizer.from_pretrained(\"allenai/scibert_scivocab_uncased\",do_lower_case=True)\ntx = list(map(lambda t : tokenizer.encode(t,padding=True,pad_to_max_length=True,max_length=250),prte))\ntx = torch.tensor(tx)\n\nind = list(pdte_new['unique_id'])\n\npred = []\nwith torch.no_grad():\n    mod.eval()\n    for i in range(0,len(tx),4):\n        l=[]\n        x = tx[i:i+4]\n        idx = ind[i:i+4]\n        x = x.to(device)\n        y_pr = mod(x,0,'pk')[0].cpu()\n        for j in range(len(x)):\n            l = [idx[j],y_pr[j].item()]\n            pred.append(l)\n        \ndf = pd.DataFrame(pred, columns = ['unique_id', 'citation_class_label']) \ndf.set_index('unique_id', inplace = True)\n\nprint(df)\ndf.to_csv('./submission.csv') ","metadata":{},"execution_count":null,"outputs":[]}]}